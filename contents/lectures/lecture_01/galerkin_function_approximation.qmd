---
jupyter: mkernel
---

```{matlab}
%| include: false
clear
setappdata(0, "MKernel_plot_backend", "inline")
setappdata(0, "MKernel_plot_format", "svg")
addpath( "../lecture_00/" )
```

# Galerkin's Method for Function Approximation

Let's suppose that we have some function that we wish to approximate as a polynomial.
Specifically, we want to find the polynomial function that most closely represents the function over a finite domain, in the $L^2$ sense.
The function we wish to approximate, over the domain $[0,1]$ is

```{=latex}
\begin{equation*}
    g(x) = \sin( \pi x )^2 + \cos( x ) - 1
\end{equation*}
```

```{matlab}
%| echo: true
%| output: false
x = sym( "x", "real" );
domain = sym( [0, 1] );
target_fun = sin( pi * x )^2 + cos( x ) - 1;
```

```{matlab}
%| echo: false
figure
fplot( target_fun, double( domain ), LineWidth=4, Color="k" )
```

And let's further suppose that we wish to approximate this function with a quartic polynomial, let's use the monomial basis

```{matlab}
%| echo: false
x = sym( "x", "real" );
domain = sym( [0, 1] );
degree = 2;
basis_name = "Monomial";

figure
hold on
basis = PolynomialBasisFunction( basis_name, degree, x, domain );
for ii = 1 : length( basis )
    fplot( basis(ii), double( domain ), LineWidth=2, DisplayName="$M_" + num2str(ii) + "$" )
end
```

```{=latex}
\begin{equation*}
    \Matrix{M}
    =
    \begin{bmatrix}
        x^0 \\
        x^1 \\
        x^2 \\
    \end{bmatrix}
\end{equation*}
```

Using the change of basis mnemonic, $\Matrix{T} \equiv \Matrix{M}$, however we need to do something different with the "from basis" and "from_coefficient" ( $\Matrix{F}$ and $\Vector{f}$, respectively).
Let's consider the right-hand side of the change of basis

```{=latex}
\begin{align}
    %%%%%%%%%%%%%
    %%% EQN 1 %%%
    %%%%%%%%%%%%%
    \Inner{\Matrix{T}}{\Matrix{F}} \Vector{f}
    &\equiv
    \begin{bmatrix}
        \Inner{\Matrix{T}_1}{\Matrix{F}_1} & \cdots & \Inner{\Matrix{T}_1}{\Matrix{F}_n} \\
        \vdots & \ddots & \vdots \\
        \Inner{\Matrix{T}_n}{\Matrix{F}_1} & \cdots & \Inner{\Matrix{T}_m}{\Matrix{F}_n}
    \end{bmatrix}
    %
    \begin{bmatrix} \Vector{f}_1 \\ \vdots \\ \Vector{f}_n \end{bmatrix} \\
    %%%%%%%%%%%%%
    %%% EQN 2 %%%
    %%%%%%%%%%%%%
    &\equiv
    \begin{bmatrix}
        \Inner{\Matrix{T}_1}{\Matrix{F}_1} \Vector{f}_1 + \cdots + \Inner{\Matrix{T}_1}{\Matrix{F}_n} \Vector{f}_n \\
        \vdots \\
        \Inner{\Matrix{T}_m}{\Matrix{F}_n} \Vector{f}_1 + \cdots + \Inner{\Matrix{T}_1}{\Matrix{F}_n} \Vector{f}_n \\
    \end{bmatrix}
\end{align}
```

Since each $\Vector{f}_j$ is a scalar, recalling the properties that comprise the definition of the inner product for real vector spaces we can rewrite each term of the form $\Inner{\Matrix{T}_i}{\Matrix{F}_j}\Vector{f_j}$ as

```{=latex}
\begin{align}
    \Inner{\Matrix{T}_i}{\Matrix{F}_j} \Vector{f}_j \equiv \Inner{\Matrix{T}_i}{\Vector{f}_j \Matrix{F}_j}
\end{align}
```

thus each row-entry of the right-hand side above can be rewritten as

```{=latex}
\begin{align}
    \Inner{\Matrix{T}_i}{\Matrix{F}_1} \Vector{f}_1 + \cdots + \Inner{\Matrix{T}_i}{\Matrix{F}_n} \Vector{f}_n &\equiv \Inner{\Matrix{T}_i}{\Vector{f}_1 \Matrix{F}_1} + \cdots + \Inner{\Matrix{T}_i}{\Vector{f}_n \Matrix{F}_n} \\
    %
    &\equiv \Inner{\Matrix{T}_i}{\Vector{f}_1 \Matrix{F}_1 + \cdots + \Vector{f}_n \Matrix{F}_n} \\
\end{align}
```

and since our target function is "defined" as

```{=latex}
\begin{equation}
    g(x) = \sum_{i=0}^{n} \Vector{f}_i \Matrix{F}_i
\end{equation}
```
 this further reduces each term to

 ```{=latex}
 \begin{equation}
     \Inner{\Matrix{T}_i}{g(x)}
 \end{equation}
 ```

 Meaning that the right hand side can be written as

 ```{=latex}
 \begin{equation}
    \Inner{\Matrix{T}}{\Matrix{F}} \Vector{f}
    \equiv
    \begin{bmatrix}
        \Inner{\Matrix{T}_1}{g(x)} \\
        \vdots \\
        \Inner{\Matrix{T}_m}{g(x)}
    \end{bmatrix}
 \end{equation}
 ```

 and our "change of basis" as

 ```{=latex}
 \begin{align}
    %%%%%%%%%%%%%
    %%% EQN 1 %%%
    %%%%%%%%%%%%%
    \Inner{\Matrix{T}}{\Matrix{T}} \Vector{t} &= \Inner{\Matrix{T}}{\Matrix{F}} \Vector{f} \\
    %%%%%%%%%%%%%
    %%% EQN 2 %%%
    %%%%%%%%%%%%%
    \begin{bmatrix}
        \Inner{\Matrix{T}_1}{\Matrix{T}_1} & \cdots & \Inner{\Matrix{T}_1}{\Matrix{T}_n} \\
        \vdots & \ddots & \vdots \\
        \Inner{\Matrix{T}_m}{\Matrix{T}_1} & \cdots & \Inner{\Matrix{T}_m}{\Matrix{T}_n}
    \end{bmatrix}
    \begin{bmatrix}
        t_0 \\ \vdots \\ t_n
    \end{bmatrix}
    &=
    \begin{bmatrix}
        \Inner{\Matrix{T}_1}{g(x)} \\
        \vdots \\
        \Inner{\Matrix{T}_n}{g(x)}
    \end{bmatrix}
 \end{align}
 ```

Proceeding with assembly of the left-hand side, which is often referred to as the *Gram matrix*

```{=latex}
\begin{align*}
    %%% EQN 1
    \Matrix{D} &= \Inner{\Matrix{M}}{\Matrix{M}} \\
    %%% EQN 2
    \Matrix{D} &=
    \begin{bmatrix}
        \int_{0}^{1} x^0 x^0 \mathop{dx} & \int_{0}^{1} x^0 x^1 \mathop{dx} & \int_{0}^{1} x^0 x^2 \mathop{dx} \\
        \int_{0}^{1} x^1 x^0 \mathop{dx} & \int_{0}^{1} x^1 x^1 \mathop{dx} & \int_{0}^{1} x^1 x^2 \mathop{dx} \\
        \int_{0}^{1} x^2 x^0 \mathop{dx} & \int_{0}^{1} x^2 x^1 \mathop{dx} & \int_{0}^{1} x^2 x^2 \mathop{dx} \\
    \end{bmatrix} \\
    %%% EQN 3
    \Matrix{D} &=
    \begin{bmatrix}
        1           & \frac{1}{2}  & \frac{1}{3} \\
        \frac{1}{2} & \frac{1}{3}  & \frac{1}{4} \\
        \frac{1}{3} & \frac{1}{4}  & \frac{1}{5} \\
    \end{bmatrix} \\
\end{align*}
```

```{matlab}
%| eval: true
%| echo: true
D = int( basis * transpose( basis ), domain );
```

And the right-hand side, often referred to as the *force vector*

```{matlab}
%| eval: true
%| echo: true
F = int( basis * target_fun, domain );
```

```{=latex}
\begin{align*}
    %%% EQN 1
    \Matrix{C}\Vector{c} = \Vector{f} &= \Inner{\Matrix{M}}{g(x)} \\
    %%% EQN 2
    \Vector{f} &=
    \begin{bmatrix}
        \int_{0}^{1} x^0 \left( \sin(\pi x)^2 + \cos(x) - 1 \right) \mathop{dx} \\
        \int_{0}^{1} x^1 \left( \sin(\pi x)^2 + \cos(x) - 1 \right) \mathop{dx} \\
        \int_{0}^{1} x^2 \left( \sin(\pi x)^2 + \cos(x) - 1 \right) \mathop{dx} \\
    \end{bmatrix} \\
    %%% EQN 3
    \Vector{f} &=
    \begin{bmatrix}
        \sin(1) - \frac{1}{2}                           \\
        \cos(1) + \sin(1) - \frac{5}{4}                 \\
        2\cos(1) - \sin(1) + \frac{2\pi^2 - 3}{12\pi^2} \\
    \end{bmatrix} \\
    %%% EQN 4
    \Vector{f} &\approx
    \begin{bmatrix}
        0.3415  \\
        0.1318  \\
        0.04714 \\
    \end{bmatrix} \\
\end{align*}
```

Resulting in the linear system of equations

```{=latex}
\begin{align*}
    %%% EQN 1
    \begin{bmatrix}
        1           & \frac{1}{2}  & \frac{1}{3} \\
        \frac{1}{2} & \frac{1}{3}  & \frac{1}{4} \\
        \frac{1}{3} & \frac{1}{4}  & \frac{1}{5} \\
    \end{bmatrix}
    \begin{bmatrix}
        d_0 \\
        d_1 \\
        d_2
    \end{bmatrix}
    &=
    \begin{bmatrix}
        0.3415  \\
        0.1318  \\
        0.04714 \\
    \end{bmatrix} \\
    %%% EQN 2
    \begin{bmatrix}
        d_0 \\
        d_1 \\
        d_2
    \end{bmatrix}
    &=
    \begin{bmatrix}
        1           & \frac{1}{2}  & \frac{1}{3} \\
        \frac{1}{2} & \frac{1}{3}  & \frac{1}{4} \\
        \frac{1}{3} & \frac{1}{4}  & \frac{1}{5} \\
    \end{bmatrix}^{-1}
    \begin{bmatrix}
        0.3415  \\
        0.1318  \\
        0.04714 \\
    \end{bmatrix} \\
    %%% EQN 2
    \begin{bmatrix}
        d_0 \\
        d_1 \\
        d_2
    \end{bmatrix}
    &\approx
    \begin{bmatrix}
        -0.2565  \\
         4.523  \\
        -4.99 \\
    \end{bmatrix}
\end{align*}
```

```{matlab}
%| eval: true
%| echo: true
d = D \ F;
```


Finally, we construct our polynomial approximation

```{=latex}
\begin{equation}
    \tilde{g}(x) = \Vector{d}^T \Matrix{M}
\end{equation}
```

```{=latex}
\begin{equation*}
    \tilde{g}(x) \approx  -4.99 x^2 + 4.523 x^1 - 0.2565 x^0
\end{equation*}
```

```{matlab}
%| eval: true
%| echo: true
approx_fun = transpose( d ) * basis;
```

```{matlab}
%| eval: true
%| echo: false
figure
hold on
fplot( target_fun, double( domain ), LineWidth=4, Color="k",     DisplayName="$f(x)$" )
fplot( approx_fun, double( domain ), LineWidth=3, SeriesIndex=1, DisplayName="$\tilde{f}(x)$" )
```

Now, that doesn't appear to be a particularly good fit, but let's look at what happens if we use a higher-order polynomial function in our approximation.

```{matlab}
%| output: false
function l2_error = ComputeL2Error( target_fun, approx_fun, domain, method )
    l2_error_fun = simplify( ( target_fun - approx_fun )^2, Steps=10 );
    if method == "exact"
        l2_error = sqrt( int( l2_error_fun, domain ) );
    elseif method == "fast"
        l2_error_fun = matlabFunction( l2_error_fun );
        l2_error_fun_vpa = @(x) double( vpa( l2_error_fun(x), 32 ) );
        domain = double( domain );
        l2_error = sqrt( integral( l2_error_fun_vpa, domain(1), domain(2), reltol=1e-12 ) );
    end
end
```

```{matlab}
l2_error = ComputeL2Error( target_fun, approx_fun, domain, "exact" );
l2_error = vpa( l2_error, 4 )
```

But first, let's collect our work above to write a function that performs the scalar projection:

```{matlab}
%| output: false
function [u, M, F, basis, d] = ScalarProjection( target_fun, basis_name, degree, domain )

    function D = AssembleGramMatrix()
        D = int( basis * transpose( basis ), domain );
    end

    function F = AssembleForceVector()
        F = int( basis * target_fun, domain );
    end

    variate = symvar( target_fun );
    if isempty( variate )
        variate = sym( "x", "real" );
    end
    basis = PolynomialBasisFunction( basis_name, degree, variate, domain );
    M = AssembleGramMatrix();
    F = AssembleForceVector();
    d = M \ F;
    u = transpose( d ) * basis;
end
```

Before we move on, I just want to point something out that I think is rather important.
Look at *how simple* the assembly operations are in the above code, and how simple Galerkin's method is when written symbolically.
In later lectures, we will complicate our implementation(s) of Galerkins method for solving various problems, by adding numerical routines that are necessary for the efficient solution of them.
In my experience, courses that dive immediately into those numerical approaches often leave students confused because of the necessity to implement all these routines before being able to see a solution.
Here, with this (and later) symbolic implementations, the student have both a simple framework within which to insert numerical algorithms as well as a tool for generating reference solutions for testing.

Now lets see what happens if we use a high-order polynomial for our approximation -- this time a quartic polynomial.

```{=latex}
\begin{equation*}
    \tilde{g}(x) = 20.42 x^4 - 40.76 x^3 + 21.15 x^2 - 1.265 x^1 + 0.0313 x^0
\end{equation*}
```

```{matlab}
%| echo: true
%| output: false
x = sym( "x", "real" );
target_fun = sin( pi * x )^2 + cos( x ) - 1;
domain = sym( [0, 1] );
degree = 4;
basis_name = "Monomial";
[u, M, F, basis, d] = ScalarProjection( target_fun, basis_name, degree, domain );
```

```{matlab}
%| echo: false
figure
hold on
fplot( target_fun, double( domain ), linewidth=4, DisplayName="$f(x)$" )
fplot( u, double( domain ),          linewidth=3, DisplayName="$\tilde{f}(x)$" )
legend()
```

```{matlab}
l2_error = ComputeL2Error( target_fun, u, domain, "exact" );
l2_error = vpa( l2_error, 4 )
```


As we can clearly see, the higher-order polynomial is a much better approximation of the function.
We can perform a "degree-refinement study" to explore and quantify the improvement of approximation as we increase the polynomial degree of the approximation.

## Convergence of the Galerkin method under degree refinement

```{matlab}
%| echo: false
x = sym( "x", "real" );
target_fun = sin( pi * x )^2 + cos( x ) - 1;
domain = sym( [0, 1] );
basis_name = "Monomial";
[u, M, F, basis, d] = ScalarProjection( target_fun, basis_name, degree, domain );

degree = 0 : 6;
l2_error = sym( zeros( size( degree ) ) );
for p = degree
    approx_fun = ScalarProjection( target_fun, basis_name, p, domain );
    l2_error(p+1) = vpa( ComputeL2Error( target_fun, approx_fun, domain, "exact" ), 32 );
end
```

```{matlab}
%| echo: false
figure
cmap = colormap( "lines" );
plot( degree, l2_error, LineWidth=2, Marker="o", MarkerFaceColor=cmap(1,:), MarkerEdgeColor="k" )
ax = gca;
xlabel( "Degree" )
ylabel( "$L^2$ Error" )
ax.YScale = "log";
```

## Conditioning

```{matlab}
%| echo: false
degree = (0 : 6);
condition = struct( "Monomial", [], "Bernstein", [], "Lagrange", [], "Legendre", [], "Chebyshev", [] );
basis_functions = string( fieldnames( condition ) );
for ii = 1 : length( basis_functions )
    basis_name = basis_functions(ii);
    for p = degree 
        basis = PolynomialBasisFunction( basis_name, p, x, domain );
        D = int( basis .* transpose( basis ), domain );
        eigvals = ComputeEigenvalues( D, 1e-12, 1e4 );
        condition.(basis_name)(end+1) = max(eigvals) / min( eigvals );
    end
end

figure
hold on
for ii = 1 : length( basis_functions )
    basis_name = basis_functions(ii);
    plot( degree, condition.(basis_name), LineWidth=2, DisplayName=basis_name )
end
ax = gca;
xlabel( "Degree" )
ylabel( "Condition Number" )
grid on
ax.YScale = "log";
legend( Location="northwest" )
```

```{matlab}
%| echo: false
[u, D, F, basis, d] = ScalarProjection( target_fun, "Monomial", 12, domain );
Dd = double( D );
Fd = double( F );
dd = inv( Dd ) * Fd;
ud = transpose( dd ) * basis;

figure
hold on
fplot( target_fun, double( domain ), LineWidth=4, Color="k" )
fplot( u, double( domain ), LineWidth=2, SeriesIndex=1 )
```