---
engine: julia
---
```{julia}
#| include: false
import Pkg
Pkg.activate( "../LearnFEM" )
import Symbolics, Latexify, SymbolicUtils, LaTeXStrings, Nemo
import LinearAlgebra
import Plots
include( "lecture_01_utils.jl" )
using .lecture_01_utils: to_numeric
include( "lecture_01.jl" )
using .lecture_01: ChangeOfVariable, PolynomialBasis, PlotPolynomialBasis, PolynomialBasisFunction, ScalarProjection, ComputeL2Error
```

# Galerkin's Method for Function Approximation

Let's suppose that we have some function that we wish to approximate as a polynomial.
Specifically, we want to find the polynomial function that most closely represents the function over a finite domain, in the $L^2$ sense.
The function we wish to approximate, over the domain $[0,1]$ is

```{julia}
#| include: true
let
    Symbolics.@variables x target_fun(x)
    target_fun(x) = sin( π * x )^2 + cos( x ) - 1;
    domain = [0, 1]
    Plots.plot( target_fun, domain[1], domain[2], linewidth=4 )
end
```

```{=latex}
\begin{equation*}
    g(x) = \sin( \pi x )^2 + \cos( x ) - 1
\end{equation*}
```

And let's further suppose that we wish to approximate this function with a quartic polynomial, let's use the monomial basis

```{julia}
#| eval: true
let
    Symbolics.@variables x
    domain = [0, 1]
    degree = 2
    basis_name = lecture_01.Monomial
    monomial_basis = PolynomialBasisFunction( basis_name, degree, x, domain )
    PlotPolynomialBasis( basis_name, degree, x, domain )
end
```

```{=latex}
\begin{equation*}
    \Matrix{M}
    =
    \begin{bmatrix}
        x^0 \\
        x^1 \\
        x^2 \\
    \end{bmatrix}
\end{equation*}
```

Using the change of basis mnemonic, $\Matrix{T} \equiv \Matrix{M}$, however we need to do something different with the "from basis" and "from_coefficient" ( $\Matrix{F}$ and $\Vector{f}$, respectively).
Let's consider the right-hand side of the change of basis

```{=latex}
\begin{align}
    %%%%%%%%%%%%%
    %%% EQN 1 %%%
    %%%%%%%%%%%%%
    \Inner{\Matrix{T}}{\Matrix{F}} \Vector{f}
    &\equiv
    \begin{bmatrix}
        \Inner{\Matrix{T}_1}{\Matrix{F}_1} & \cdots & \Inner{\Matrix{T}_1}{\Matrix{F}_n} \\
        \vdots & \ddots & \vdots \\
        \Inner{\Matrix{T}_n}{\Matrix{F}_1} & \cdots & \Inner{\Matrix{T}_m}{\Matrix{F}_n}
    \end{bmatrix}
    %
    \begin{bmatrix} \Vector{f}_1 \\ \vdots \\ \Vector{f}_n \end{bmatrix} \\
    %%%%%%%%%%%%%
    %%% EQN 2 %%%
    %%%%%%%%%%%%%
    &\equiv
    \begin{bmatrix}
        \Inner{\Matrix{T}_1}{\Matrix{F}_1} \Vector{f}_1 + \cdots + \Inner{\Matrix{T}_1}{\Matrix{F}_n} \Vector{f}_n \\
        \vdots \\
        \Inner{\Matrix{T}_m}{\Matrix{F}_n} \Vector{f}_1 + \cdots + \Inner{\Matrix{T}_1}{\Matrix{F}_n} \Vector{f}_n \\
    \end{bmatrix}
\end{align}
```

Since each $\Vector{f}_j$ is a scalar, recalling the properties that comprise the definition of the inner product for real vector spaces we can rewrite each term of the form $\Inner{\Matrix{T}_i}{\Matrix{F}_j}\Vector{f_j}$ as

```{=latex}
\begin{align}
    \Inner{\Matrix{T}_i}{\Matrix{F}_j} \Vector{f}_j \equiv \Inner{\Matrix{T}_i}{\Vector{f}_j \Matrix{F}_j}
\end{align}
```

thus each row-entry of the right-hand side above can be rewritten as

```{=latex}
\begin{align}
    \Inner{\Matrix{T}_i}{\Matrix{F}_1} \Vector{f}_1 + \cdots + \Inner{\Matrix{T}_i}{\Matrix{F}_n} \Vector{f}_n &\equiv \Inner{\Matrix{T}_i}{\Vector{f}_1 \Matrix{F}_1} + \cdots + \Inner{\Matrix{T}_i}{\Vector{f}_n \Matrix{F}_n} \\
    %
    &\equiv \Inner{\Matrix{T}_i}{\Vector{f}_1 \Matrix{F}_1 + \cdots + \Vector{f}_n \Matrix{F}_n} \\
\end{align}
```

and since our target function is "defined" as

```{=latex}
\begin{equation}
    g(x) = \sum_{i=0}^{n} \Vector{f}_i \Matrix{F}_i
\end{equation}
```
 this further reduces each term to

 ```{=latex}
 \begin{equation}
     \Inner{\Matrix{T}_i}{g(x)}
 \end{equation}
 ```

 Meaning that the right hand side can be written as

 ```{=latex}
 \begin{equation}
    \Inner{\Matrix{T}}{\Matrix{F}} \Vector{f}
    \equiv
    \begin{bmatrix}
        \Inner{\Matrix{T}_1}{g(x)} \\
        \vdots \\
        \Inner{\Matrix{T}_m}{g(x)}
    \end{bmatrix}
 \end{equation}
 ```

 and our "change of basis" as

 ```{=latex}
 \begin{align}
    %%%%%%%%%%%%%
    %%% EQN 1 %%%
    %%%%%%%%%%%%%
    \Inner{\Matrix{T}}{\Matrix{T}} \Vector{t} &= \Inner{\Matrix{T}}{\Matrix{F}} \Vector{f} \\
    %%%%%%%%%%%%%
    %%% EQN 2 %%%
    %%%%%%%%%%%%%
    \begin{bmatrix}
        \Inner{\Matrix{T}_1}{\Matrix{T}_1} & \cdots & \Inner{\Matrix{T}_1}{\Matrix{T}_n} \\
        \vdots & \ddots & \vdots \\
        \Inner{\Matrix{T}_n}{\Matrix{T}_1} & \cdots & \Inner{\Matrix{T}_m}{\Matrix{T}_n}
    \end{bmatrix}
    &=
    \begin{bmatrix}
        \Inner{\Matrix{T}_1}{g(x)} \\
        \vdots \\
        \Inner{\Matrix{T}_m}{g(x)}
    \end{bmatrix}
 \end{align}
 ```

Proceeding with assembly of the left-hand side, which is often referred to as the *Gram matrix*

```{=latex}
\begin{align*}
    %%% EQN 1
    \Matrix{D} &= \Inner{\Matrix{M}}{\Matrix{M}} \\
    %%% EQN 2
    \Matrix{D} &=
    \begin{bmatrix}
        \int_{0}^{1} x^0 x^0 \mathop{dx} & \int_{0}^{1} x^0 x^1 \mathop{dx} & \int_{0}^{1} x^0 x^2 \mathop{dx} \\
        \int_{0}^{1} x^1 x^0 \mathop{dx} & \int_{0}^{1} x^1 x^1 \mathop{dx} & \int_{0}^{1} x^1 x^2 \mathop{dx} \\
        \int_{0}^{1} x^2 x^0 \mathop{dx} & \int_{0}^{1} x^2 x^1 \mathop{dx} & \int_{0}^{1} x^2 x^2 \mathop{dx} \\
    \end{bmatrix} \\
    %%% EQN 3
    \Matrix{D} &=
    \begin{bmatrix}
        1           & \frac{1}{2}  & \frac{1}{3} \\
        \frac{1}{2} & \frac{1}{3}  & \frac{1}{4} \\
        \frac{1}{3} & \frac{1}{4}  & \frac{1}{5} \\
    \end{bmatrix} \\
\end{align*}
```

And the right-hand side, often referred to as the *force vector*

```{julia}
#| eval: true
#| echo: false
let
    using SymbolicNumericIntegration: integrate

    Symbolics.@variables x
    target_fun = sin( π * x )^2 + cos( x ) - 1;
    domain = [0, 1]
    degree = 2
    basis_name = lecture_01.Monomial
    monomial_basis = PolynomialBasisFunction( basis_name, degree, x, domain )
    
    var_tuple = fill( ( x, domain[1], domain[2] ), ( degree + 1, degree + 1 ) )
    integrand = monomial_basis * transpose( monomial_basis )
    D = integrate.( integrand )
    D = integrate.( integrand, var_tuple; detailed=false )
    D = to_numeric( Float64, D)

    var_tuple = fill( ( x, domain[1], domain[2] ), ( degree + 1, 1 ) )
    integrand = monomial_basis * target_fun
    F = integrate.( integrand )
    F = integrate.( integrand, var_tuple; detailed=false, symbolic=true,abstol=1e-20, num_steps=10, num_trials=100, verbose=true )
    d = vec( D \ F )
    approx_fun = transpose( d ) * monomial_basis

    Plots.plot( target_fun, domain[1], domain[2], linewidth=4, label="" )
    Plots.plot!( approx_fun, domain[1], domain[2], linewidth=3, label="" )
end
```

```{=latex}
\begin{align*}
    %%% EQN 1
    \Matrix{C}\Vector{c} = \Vector{f} &= \Inner{\Matrix{M}}{g(x)} \\
    %%% EQN 2
    \Vector{f} &=
    \begin{bmatrix}
        \int_{0}^{1} x^0 \left( \sin(\pi x)^2 + \cos(x) - 1 \right) \mathop{dx} \\
        \int_{0}^{1} x^1 \left( \sin(\pi x)^2 + \cos(x) - 1 \right) \mathop{dx} \\
        \int_{0}^{1} x^2 \left( \sin(\pi x)^2 + \cos(x) - 1 \right) \mathop{dx} \\
    \end{bmatrix} \\
    %%% EQN 3
    \Vector{f} &=
    \begin{bmatrix}
        \sin(1) - \frac{1}{2}                           \\
        \cos(1) + \sin(1) - \frac{5}{4}                 \\
        2\cos(1) - \sin(1) + \frac{2\pi^2 - 3}{12\pi^2} \\
    \end{bmatrix} \\
    %%% EQN 4
    \Vector{f} &\approx
    \begin{bmatrix}
        0.3415  \\
        0.1318  \\
        0.04714 \\
    \end{bmatrix} \\
\end{align*}
```

Resulting in the linear system of equations

```{=latex}
\begin{align*}
    %%% EQN 1
    \begin{bmatrix}
        1           & \frac{1}{2}  & \frac{1}{3} \\
        \frac{1}{2} & \frac{1}{3}  & \frac{1}{4} \\
        \frac{1}{3} & \frac{1}{4}  & \frac{1}{5} \\
    \end{bmatrix}
    \begin{bmatrix}
        d_0 \\
        d_1 \\
        d_2
    \end{bmatrix}
    &=
    \begin{bmatrix}
        0.3415  \\
        0.1318  \\
        0.04714 \\
    \end{bmatrix} \\
    %%% EQN 2
    \begin{bmatrix}
        d_0 \\
        d_1 \\
        d_2
    \end{bmatrix}
    &=
    \begin{bmatrix}
        1           & \frac{1}{2}  & \frac{1}{3} \\
        \frac{1}{2} & \frac{1}{3}  & \frac{1}{4} \\
        \frac{1}{3} & \frac{1}{4}  & \frac{1}{5} \\
    \end{bmatrix}^{-1}
    \begin{bmatrix}
        0.3415  \\
        0.1318  \\
        0.04714 \\
    \end{bmatrix} \\
    %%% EQN 2
    \begin{bmatrix}
        d_0 \\
        d_1 \\
        d_2
    \end{bmatrix}
    &\approx
    \begin{bmatrix}
        -0.2565  \\
         4.523  \\
        -4.99 \\
    \end{bmatrix}
\end{align*}
```

Finally, we construct our polynomial approximation

```{=latex}
\begin{equation}
    \tilde{g}(x) = \Vector{d}^T \Matrix{M}
\end{equation}
```

```{=latex}
\begin{equation*}
    \tilde{g}(x) \approx  -4.99 x^2 + 4.523 x^1 - 0.2565 x^0
\end{equation*}
```

Now, that doesn't appear to be a particularly good fit, but let's look at what happens if we use a higher-order polynomial function in our approximation.
But first, let's collect our work above to write a simple class that performs the scalar projection:

```{julia}
#| eval: false
using SymbolicNumericIntegration: integrate
function ScalarProjection( target_fun, basis_name, degree, domain )
    integrate_params = Dict(
        :abstol     => 1e-12,
        :detailed   => false,
        :symbolic   => false,
    )

    function AssembleGramMatrix()
        var_tuple = fill( ( variate, domain[1], domain[2] ), ( basis_dim, basis_dim ) )
        D = integrate.( basis * transpose( basis ), var_tuple; integrate_params... )
        return D
    end

    function AssembleForceVector( target_fun, basis, domain, variate )
        var_tuple = fill( ( variate, domain[1], domain[2] ), ( basis_dim, 1 ) )
        F = integrate.( basis * target_fun, var_tuple; integrate_params... )
        return F
    end

    variate = Symbolics.Num( Symbolics.get_variables( target_fun )[1] )
    basis = PolynomialBasisFunction( basis_name, degree, variate, domain )
    basis_dim = length( basis )
    M = AssembleGramMatrix( )
    F = AssembleForceVector( )
    d = vec( M \ F )
    u = transpose( d ) * basis
    return u, M, F, basis, d
end
```

Now lets see what happens if we use a high-order polynomial for our approximation -- this time a quartic polynomial

```{=latex}
\begin{equation*}
    \tilde{g}(x) = 20.42 x^4 - 40.76 x^3 + 21.15 x^2 - 1.265 x^1 + 0.0313 x^0
\end{equation*}
```

```{julia}
#| echo: false
let
    Symbolics.@variables x target_fun(x)
    target_fun = sin( π * x )^2 + cos( x ) - 1
    degree = 4
    domain = [0, 1]
    basis_name = lecture_01.Monomial
    u, M, F, basis, d = ScalarProjection( target_fun, basis_name, degree, domain )
    Plots.plot( target_fun, domain[1], domain[2], linewidth=4, label="" )
    Plots.plot!( u, domain[1], domain[2], linewidth=3, label="" )
end
```

As we can clearly see, the higher-order polynomial is a much better approximation of the function.
We can perform a "degree-refinement study" to explore and quantify the improvement of approximation as we increase the polynomial degree of the approximation.

## Convergence of the Galerkin method under degree refinement

```{julia}
#| echo: false
let
    Symbolics.@variables x
    target_fun = sin( π * x )^2 + cos( x ) - 1
    domain = [0, 1]
    basis_name = lecture_01.Legendre

    l2_error = []
    degree = 0 : 5
    for p in degree
        approx_fun = ScalarProjection( target_fun, basis_name, p, domain )[1]
        err = ComputeL2Error( target_fun, approx_fun, domain )
        append!( l2_error, err )
    end

    Plots.plot( degree, l2_error, markershape=:circle, linewidth=2 )
    Plots.plot!( yscale=:log10, minorgrid=true )
    Plots.xlabel!( "Degree" )
    Plots.ylabel!( "L2 Error" )
end
```

```{python}
#| echo: false
#| warning: false
#| error: false

plt = matplotlib.pyplot.plot( degree, l2_error )
matplotlib.pyplot.xlabel( "Degree" )
matplotlib.pyplot.ylabel( "$L^2 Error$" )
matplotlib.pyplot.yscale( "log" )
matplotlib.pyplot.show()
```

## Conditioning

```{python}
#| echo: false
#| warning: false
#| error: false

cond = { "Monomial": [], "Bernstein": [], "Lagrange": [], "Legendre": [], "Chebyshev": [] }
for basis_name in list( cond.keys() ):
    for i in range( 0, len( degree ) ):
        p = degree[i]
        basis = PolynomialBasisFunction( basis_name, p, x, domain )
        basis = BasisPolynomialListToLambdaArray( basis, x )
        D = ScalarProjectionFast().AssembleGramMatrix( basis, domain )
        cond[basis_name].append( numpy.linalg.cond( D ) )
    matplotlib.pyplot.plot( degree, cond[basis_name], label=basis_name )
matplotlib.pyplot.xlabel( "Degree" )
matplotlib.pyplot.ylabel( "$L^2 Error$" )
matplotlib.pyplot.yscale( "log" )
matplotlib.pyplot.legend()
matplotlib.pyplot.show()
```

```{python}
#| echo: false
u, D, F, _, _ = ScalarProjectionFast()( target_fun, "Monomial", 12, domain )
xd = numpy.linspace( float( domain[0] ), float( domain[1] ), 1000 )
target_fun_lambda = sympy.lambdify( x, target_fun )
target_fun_d = numpy.array( [target_fun_lambda( xd[i] ) for i in range( 0, len( xd ) )] )
ud = numpy.array( [u( xd[i] ) for i in range( 0, len( xd ) )] )

matplotlib.pyplot.plot( xd, target_fun_d )
matplotlib.pyplot.plot( xd, ud )
matplotlib.pyplot.show()
```